{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a670e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 22 03:49:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:10.0 Off |                    0 |\n",
      "| N/A   29C    P0             41W /  400W |      14MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            1332      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290f4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import random\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5bedf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-22 04:02:11,528 - WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-05-22 04:02:11,533 - INFO - Successfully authenticated with Hugging Face\n",
      "2025-05-22 04:02:11,535 - INFO - WANDB_API_KEY set successfully\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Authenticate with Hugging Face using environment variables\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    logger.info(\"Successfully authenticated with Hugging Face\")\n",
    "else:\n",
    "    logger.warning(\"HF_TOKEN not found in environment variables. Some operations may fail.\")\n",
    "    \n",
    "# Set Wandb API key from environment variables\n",
    "wandb_api_key = os.environ.get(\"WANDB_API_KEY\")\n",
    "if wandb_api_key:\n",
    "    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "    logger.info(\"WANDB_API_KEY set successfully\")\n",
    "else:\n",
    "    logger.warning(\"WANDB_API_KEY not found in environment variables. Wandb logging may not work.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93ae970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/ubuntu/TrOCR/lib/python3.8/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from bitsandbytes) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from scipy->bitsandbytes) (1.24.3)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/TrOCR/lib/python3.8/site-packages (0.28.0)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from accelerate) (2.4.1+cu121)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from accelerate) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.67.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.10.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.2.3)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/TrOCR/lib/python3.8/site-packages (4.38.2)\n",
      "Requirement already satisfied: torch in /home/ubuntu/TrOCR/lib/python3.8/site-packages (2.4.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (12.8.61)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/TrOCR/lib/python3.8/site-packages (2.19.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (0.70.17)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0; python_version < \"3.11\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/TrOCR/lib/python3.8/site-packages (4.67.1)\n",
      "Requirement already satisfied: wandb in /home/ubuntu/TrOCR/lib/python3.8/site-packages (0.13.10)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: PyYAML in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (44.0.0)\n",
      "Requirement already satisfied: setproctitle in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0; python_version < \"3.9\" and sys_platform == \"linux\" in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from wandb) (4.25.6)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.11 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from sentry-sdk>=1.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from sentry-sdk>=1.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/TrOCR/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: python-dotenv in /home/ubuntu/TrOCR/lib/python3.8/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install transformers torch\n",
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install wandb\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration class to store all training parameters\n",
    "    \"\"\"\n",
    "    # Model parameters\n",
    "    SOURCE_MODEL = \"facebook/nllb-200-1.3B\"  # Teacher model\n",
    "    TARGET_MODEL = \"facebook/nllb-200-distilled-600M\"  # Student model\n",
    "    SOURCE_LANG = \"eng_Latn\"  # English\n",
    "    TARGET_LANG = \"khm_Khmr\"  # Khmer\n",
    "    TARGET_LANG_TOKEN_ID = 256092  # Khmer language token ID\n",
    "    \n",
    "    # Distillation parameters\n",
    "    TARGET_ENCODER_LAYERS = 3  # Number of encoder layers to keep\n",
    "    TARGET_DECODER_LAYERS = 3  # Number of decoder layers to keep\n",
    "    TEMPERATURE = 5  # Temperature for knowledge distillation\n",
    "    LAMBDA_PARAM = 0.5  # Weight balancing between student loss and distillation loss\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 48\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    WARMUP_RATIO = 0.06\n",
    "    NUM_EPOCHS = 1\n",
    "    LEARNING_RATE = 3e-5\n",
    "    FP16 = True # Set False for standard precision; set True for GPU speedup\n",
    "    LOGGING_STEPS = 1000\n",
    "    OPTIM = \"adamw_torch\"\n",
    "    EVAL_STRATEGY = 'epoch'\n",
    "    SAVE_STRATEGY = \"epoch\"\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    LR_SCHEDULER_TYPE = 'cosine'\n",
    "    LOAD_BEST_MODEL_AT_END = True  # Changed to True to save best model\n",
    "    SAVE_TOTAL_LIMIT = 1\n",
    "    DDP_FIND_UNUSED_PARAMETERS = False\n",
    "    GROUP_BY_LENGTH = False\n",
    "    REPORT_TO = 'wandb'\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    EARLY_STOPPING_PATIENCE = 3\n",
    "    EARLY_STOPPING_THRESHOLD = 0.01\n",
    "    \n",
    "    # Data parameters\n",
    "    MAX_SEQ_LENGTH = 128\n",
    "    DATA_PATH = os.environ.get(\"TRAINING_DATA_PATH\", \"/home/ubuntu/distill/316K-synthetic.json\")\n",
    "    \n",
    "    # Output parameters\n",
    "    OUTPUT_DIR = './nllb_350M'\n",
    "    MODEL_NAME = \"nllb_350M_en_km_v1\"\n",
    "    WANDB_PROJECT = \"NLLB_DISTILLATION_v1\"\n",
    "\n",
    "# Create config instance\n",
    "config = Config()\n",
    "\n",
    "# Configure Wandb\n",
    "os.environ[\"WANDB_PROJECT\"] = config.WANDB_PROJECT\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = config.MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0bddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 03:49:51,840 - INFO - Loading teacher model from facebook/nllb-200-1.3B\n",
      "/home/ubuntu/TrOCR/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-05-22 03:49:53,735 - INFO - Loading student model from facebook/nllb-200-distilled-600M\n",
      "2025-05-22 03:49:54,833 - INFO - Loading tokenizer from facebook/nllb-200-distilled-600M\n",
      "2025-05-22 03:49:56,473 - INFO - Reducing model size: keeping 3 encoder layers and 3 decoder layers\n",
      "2025-05-22 03:49:56,511 - INFO - Setting forced BOS token ID to 256092 (Khmer)\n",
      "2025-05-22 03:49:56,514 - INFO - Student model has 350.54M parameters\n"
     ]
    }
   ],
   "source": [
    "def create_distilled_model(teacher_model_name, student_model_name, num_encoder_layers, num_decoder_layers, target_lang_token_id):\n",
    "    \"\"\"\n",
    "    Initialize teacher and student models, and prepare the student model for distillation\n",
    "    \n",
    "    Args:\n",
    "        teacher_model_name (str): Hugging Face model ID for the teacher model\n",
    "        student_model_name (str): Hugging Face model ID for the student model\n",
    "        num_encoder_layers (int): Number of encoder layers to keep in the student model\n",
    "        num_decoder_layers (int): Number of decoder layers to keep in the student model\n",
    "        target_lang_token_id (int): Token ID for the target language\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (teacher_model, student_model, tokenizer)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading teacher model from {teacher_model_name}\")\n",
    "    teacher_model = AutoModelForSeq2SeqLM.from_pretrained(teacher_model_name)\n",
    "    \n",
    "    logger.info(f\"Loading student model from {student_model_name}\")\n",
    "    student_model = AutoModelForSeq2SeqLM.from_pretrained(student_model_name)\n",
    "    \n",
    "    logger.info(f\"Loading tokenizer from {student_model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        student_model_name, \n",
    "        src_lang=config.SOURCE_LANG, \n",
    "        tgt_lang=config.TARGET_LANG\n",
    "    )\n",
    "    \n",
    "    # Reduce the model size by keeping only selected layers\n",
    "    logger.info(f\"Reducing model size: keeping {num_encoder_layers} encoder layers and {num_decoder_layers} decoder layers\")\n",
    "    student_model.model.encoder.layers = student_model.model.encoder.layers[:num_encoder_layers]\n",
    "    student_model.model.decoder.layers = student_model.model.decoder.layers[:num_decoder_layers]\n",
    "    \n",
    "    # Update the model config to reflect the reduced architecture\n",
    "    student_model.config.encoder_layers = num_encoder_layers\n",
    "    student_model.config.decoder_layers = num_decoder_layers\n",
    "    \n",
    "    # Set forced BOS token ID for both models\n",
    "    logger.info(f\"Setting forced BOS token ID to {target_lang_token_id} (Khmer)\")\n",
    "    teacher_model.config.forced_bos_token_id = target_lang_token_id\n",
    "    student_model.config.forced_bos_token_id = target_lang_token_id\n",
    "    \n",
    "    # Log model size\n",
    "    num_params = student_model.num_parameters() / 1_000_000\n",
    "    logger.info(f\"Student model has {num_params:.2f}M parameters\")\n",
    "    \n",
    "    return teacher_model, student_model, tokenizer\n",
    "\n",
    "# Initialize models\n",
    "teacher_model, model, tokenizer = create_distilled_model(\n",
    "    config.SOURCE_MODEL,\n",
    "    config.TARGET_MODEL,\n",
    "    config.TARGET_ENCODER_LAYERS,\n",
    "    config.TARGET_DECODER_LAYERS,\n",
    "    config.TARGET_LANG_TOKEN_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a0c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 03:49:56,540 - INFO - Loading training data from /home/ubuntu/distill/316K-synthetic.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcd84f5a3f142d5becd7abba8ccf643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 03:50:03,208 - INFO - Loading ALT dataset for evaluation\n",
      "2025-05-22 03:50:08,756 - INFO - Available ALT dataset splits: ['train', 'validation', 'test']\n",
      "2025-05-22 03:50:08,757 - INFO - Processing train split\n",
      "2025-05-22 03:50:10,098 - INFO - Processing validation split\n",
      "2025-05-22 03:50:10,172 - INFO - Processing test split\n",
      "2025-05-22 03:50:14,710 - INFO - Tokenizing training dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d40f99f0ad4b3c9dc856d8c769755e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training data:   0%|          | 0/316110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 03:54:09,735 - INFO - Tokenizing evaluation dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99af088f20a4487782b908b774e1c477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing evaluation data:   0%|          | 0/20106 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 03:54:22,047 - INFO - Training dataset size: 316110\n",
      "2025-05-22 03:54:22,049 - INFO - Evaluation dataset size: 20106\n"
     ]
    }
   ],
   "source": [
    "def data_prepare(dataset):\n",
    "    \"\"\"\n",
    "    Tokenize the dataset for training and evaluation\n",
    "    \n",
    "    Args:\n",
    "        dataset (dict): Dataset containing source and target text pairs\n",
    "        \n",
    "    Returns:\n",
    "        dict: Tokenized model inputs\n",
    "    \"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        dataset[config.SOURCE_LANG], \n",
    "        text_target=dataset[config.TARGET_LANG], \n",
    "        max_length=config.MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def load_and_prepare_datasets(data_path):\n",
    "    \"\"\"\n",
    "    Load and prepare datasets for training and evaluation\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the training data file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_dataset, eval_dataset)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the training dataset\n",
    "        logger.info(f\"Loading training data from {data_path}\")\n",
    "        train_dataset = load_dataset(\"json\", data_files=data_path)\n",
    "        \n",
    "        # Load the ALT dataset for evaluation\n",
    "        logger.info(\"Loading ALT dataset for evaluation\")\n",
    "        alt_dataset = load_dataset(\"mutiyama/alt\")\n",
    "        \n",
    "        # Debug dataset structure\n",
    "        logger.info(f\"Available ALT dataset splits: {list(alt_dataset.keys())}\")\n",
    "        \n",
    "        # Create evaluation data with English-Khmer pairs from all splits\n",
    "        eval_data = []\n",
    "        for split_name in alt_dataset.keys():\n",
    "            logger.info(f\"Processing {split_name} split\")\n",
    "            for item in alt_dataset[split_name]:\n",
    "                # 'translation' is a dict with language codes as keys\n",
    "                translations = item.get(\"translation\", {})\n",
    "                if \"en\" in translations and \"khm\" in translations and translations[\"en\"] and translations[\"khm\"]:\n",
    "                    eval_data.append({\n",
    "                        config.SOURCE_LANG: translations[\"en\"],\n",
    "                        config.TARGET_LANG: translations[\"khm\"]\n",
    "                    })\n",
    "        \n",
    "        # Extract training data\n",
    "        try:\n",
    "            train_data = train_dataset[\"train\"][\"train\"][0][\"data\"]\n",
    "            train_dataset = Dataset.from_list(train_data)\n",
    "        except (KeyError, IndexError) as e:\n",
    "            logger.error(f\"Error extracting training data: {e}\")\n",
    "            raise ValueError(\"Training data format is not as expected. Please check the JSON structure.\")\n",
    "\n",
    "        # Create the evaluation dataset\n",
    "        eval_dataset = Dataset.from_list(eval_data)\n",
    "        \n",
    "        # Apply the same processing to both datasets\n",
    "        logger.info(\"Tokenizing training dataset\")\n",
    "        train_dataset = train_dataset.map(\n",
    "            data_prepare, \n",
    "            remove_columns=train_dataset.column_names,\n",
    "            desc=\"Processing training data\"\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Tokenizing evaluation dataset\")\n",
    "        if len(eval_data) > 0:\n",
    "            eval_dataset = eval_dataset.map(\n",
    "                data_prepare, \n",
    "                remove_columns=eval_dataset.column_names,\n",
    "                desc=\"Processing evaluation data\"\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "        logger.info(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "        \n",
    "        if len(eval_dataset) == 0:\n",
    "            logger.warning(\"Evaluation dataset is empty! Consider using a different evaluation dataset.\")\n",
    "            \n",
    "        return train_dataset, eval_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading datasets: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load and prepare datasets\n",
    "train_dataset, eval_dataset = load_and_prepare_datasets(config.DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4324616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"\n",
    "    Custom trainer for knowledge distillation from a teacher model to a student model.\n",
    "    \n",
    "    This trainer implements temperature-based knowledge distillation, combining\n",
    "    supervised learning loss with distillation loss during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the distillation trainer.\n",
    "        \n",
    "        Args:\n",
    "            teacher_model: Pre-trained model to use as the teacher\n",
    "            student_model: Smaller model to be trained through distillation\n",
    "            temperature: Temperature parameter for softening probability distributions\n",
    "            lambda_param: Weight balancing supervised loss vs distillation loss\n",
    "            *args: Additional arguments passed to Seq2SeqTrainer\n",
    "            **kwargs: Additional keyword arguments passed to Seq2SeqTrainer\n",
    "        \"\"\"\n",
    "        super().__init__(model=student_model, *args, **kwargs)\n",
    "        \n",
    "        # Set device and store as instance variable\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"DistilTrainer using device: {self.device}\")\n",
    "        \n",
    "        # Move models to device and update references\n",
    "        self.teacher = teacher_model.to(self.device)\n",
    "        self.student = student_model.to(self.device)\n",
    "        \n",
    "        # Move loss function to device\n",
    "        self.loss_function = nn.CrossEntropyLoss().to(self.device)\n",
    "        \n",
    "        # Set teacher to evaluation mode\n",
    "        self.teacher.eval()\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.temperature = temperature\n",
    "        self.lambda_param = lambda_param\n",
    "        \n",
    "        # Verify models are on correct device\n",
    "        logger.info(f\"Teacher model device: {next(self.teacher.parameters()).device}\")\n",
    "        logger.info(f\"Student model device: {next(self.student.parameters()).device}\")\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Compute combined loss using both supervised and distillation objectives.\n",
    "        \n",
    "        Args:\n",
    "            model: The student model being trained\n",
    "            inputs: Input data dictionary containing 'input_ids', 'attention_mask', etc.\n",
    "            return_outputs: Whether to return model outputs along with the loss\n",
    "            num_items_in_batch: Number of items in the current batch (unused)\n",
    "            \n",
    "        Returns:\n",
    "            Loss tensor or tuple of (loss, outputs) if return_outputs is True\n",
    "        \"\"\"\n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "        \n",
    "        # Get student model outputs\n",
    "        student_output = model(**inputs)\n",
    "        \n",
    "        # Get teacher model outputs (without gradient computation)\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher(**inputs)\n",
    "        \n",
    "        # Apply temperature scaling and compute softmax distributions\n",
    "        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\n",
    "        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n",
    "        \n",
    "        # Compute distillation loss and scale by temperature squared\n",
    "        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "        \n",
    "        # Get standard supervised learning loss\n",
    "        student_target_loss = student_output.loss\n",
    "        \n",
    "        # Combine the two losses with lambda weighting\n",
    "        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss\n",
    "        \n",
    "        return (loss, student_output) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "748fdefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_arguments():\n",
    "    \"\"\"\n",
    "    Create training arguments for the Seq2SeqTrainer\n",
    "    \n",
    "    Returns:\n",
    "        Seq2SeqTrainingArguments: Training arguments object\n",
    "    \"\"\"\n",
    "    # Add early stopping callback if enabled\n",
    "    callbacks = []\n",
    "    if config.EARLY_STOPPING_PATIENCE > 0:\n",
    "        from transformers.trainer_callback import EarlyStoppingCallback\n",
    "        callbacks.append(\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=config.EARLY_STOPPING_PATIENCE,\n",
    "                early_stopping_threshold=config.EARLY_STOPPING_THRESHOLD\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Set up training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=max(1, min(8, len(eval_dataset))),\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        remove_unused_columns=False,\n",
    "        warmup_ratio=config.WARMUP_RATIO,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        fp16=config.FP16,\n",
    "        logging_steps=config.LOGGING_STEPS,\n",
    "        optim=config.OPTIM,\n",
    "        evaluation_strategy=config.EVAL_STRATEGY,  \n",
    "        save_strategy=config.SAVE_STRATEGY,\n",
    "        max_grad_norm=config.MAX_GRAD_NORM,\n",
    "        lr_scheduler_type=config.LR_SCHEDULER_TYPE,\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        load_best_model_at_end=config.LOAD_BEST_MODEL_AT_END,\n",
    "        save_total_limit=config.SAVE_TOTAL_LIMIT,\n",
    "        ddp_find_unused_parameters=config.DDP_FIND_UNUSED_PARAMETERS,\n",
    "        group_by_length=config.GROUP_BY_LENGTH,\n",
    "        report_to=config.REPORT_TO,\n",
    "    )\n",
    "    \n",
    "    return args, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b90d2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/TrOCR/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/TrOCR/lib/python3.8/site-packages/accelerate/accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "2025-05-22 03:54:23,590 - INFO - DistilTrainer using device: cuda\n",
      "2025-05-22 03:54:24,814 - INFO - Teacher model device: cuda:0\n",
      "2025-05-22 03:54:24,816 - INFO - Student model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create training arguments\n",
    "training_args, callbacks = create_training_arguments()\n",
    "\n",
    "# Create trainer\n",
    "trainer = DistilTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=model,\n",
    "    temperature=config.TEMPERATURE,\n",
    "    lambda_param=config.LAMBDA_PARAM,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=callbacks,\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, \n",
    "        pad_to_multiple_of=8, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6000f0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 04:02:15,913 - INFO - Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6586' max='6586' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6586/6586 3:02:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.813100</td>\n",
       "      <td>0.808127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'forced_bos_token_id': 256092}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "2025-05-22 07:04:28,435 - INFO - Training completed successfully\n",
      "2025-05-22 07:04:28,437 - INFO - Pushing model to Hugging Face Hub as nllb_350M_en_km_v1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce8ff26336041ef8cb86985ce7ae061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'forced_bos_token_id': 256092}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bd44dc75634879826ecab1f75b427e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 07:05:49,329 - INFO - Model pushed successfully: https://huggingface.co/lyfeyvutha/nllb_350M_en_km_v1/commit/1e76c33107d06e6715b6adfc57b3ee1dbe3df19d\n"
     ]
    }
   ],
   "source": [
    "def train_and_export_model(trainer, model, export_name, private=True, resume_from_checkpoint=False):\n",
    "    \"\"\"\n",
    "    Train the model and export it to Hugging Face Hub\n",
    "    \n",
    "    Args:\n",
    "        trainer: Trainer instance\n",
    "        model: Model to be exported\n",
    "        export_name: Name for the exported model on HF Hub\n",
    "        private: Whether to make the model private on HF Hub\n",
    "        resume_from_checkpoint: Whether to resume from checkpoint if available\n",
    "        \n",
    "    Returns:\n",
    "        Model: Trained model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to resume from checkpoint if specified\n",
    "        logger.info(\"Starting training...\")\n",
    "        trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "        logger.info(\"Training completed successfully\")\n",
    "        \n",
    "        # Push model to Hub\n",
    "        logger.info(f\"Pushing model to Hugging Face Hub as {export_name}\")\n",
    "        result = model.push_to_hub(export_name, private=private)\n",
    "        logger.info(f\"Model pushed successfully: {result}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during training or export: {e}\")\n",
    "        raise\n",
    "\n",
    "# Train and export the model\n",
    "trained_model = train_and_export_model(\n",
    "    trainer, \n",
    "    model,\n",
    "    config.MODEL_NAME, \n",
    "    private=True,\n",
    "    resume_from_checkpoint=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvname)",
   "language": "python",
   "name": "venvname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
